\chapter{おわりに}
\section{なぜうまくいかなかったのか？}
今回の実験では，実験1・実験2を通じて満足のいく
結果を得ることができなかった．
うまくいかなかった原因はなぜだろうか？

実験2において，学習データ数を変化させて同じ実験を行っても
結果が大きく変化しなかったことから，
{\bf 学習がうまくいかない原因はデータ数ではない}ことがわかる．

データ数が原因でない場合，うまくいかない原因として，
\begin{itemize}
  \item 機械学習モデルが単純すぎる
  \item 特徴量抽出や課題設定に誤りがある
  \item モデルの評価手法に誤りがある
\end{itemize}
ことが考えられる．

モデルが単純すぎる場合，機械学習は
データを増やしてもうまくいかない．
しかし，今回の実験ではDNNという比較的複雑なモデルを用いている．
また，モデル構造を変化させても結果は大きく変わらなかったことから，
モデルが単純でないことは原因ではないように思える．

今回の実験では，特徴量抽出にFaceNetを用いた．
これは顔認証などにも用いられる，顔を対象とする
転移学習モデルとしては大変優れたものであった．
けれども，「顔から印象を推定する」というタスクに最適であったかはわからない．

しかしそれ以上に，問題があったのは課題設定ではないだろうか．
今回の課題は，「{\bf 人間が他人の顔に抱く印象は，
すべての人間で共通である．}」という前提条件のものでのみ成り立っている．
ある人の顔を見た時に，Aさんは「信頼できる」と感じ，
Bさんは「信頼できない」と感じるならば，そもそも今回の課題は成立しない．

\subsection{課題設定の前提条件を検証する}
「{\bf 人間が他人の顔に抱く印象は，
すべての人間で共通である．}」という前提条件が正しかったのかを
考える．幸いなことに，実験1のクラウドソーシングでは
1タスク3ワーカーで行われており，このデータを用いて
前提条件を検証できる\footnote{
  本来は実験1のクラウドソーシングを行った後に検証し，
  方向性を修正すべきだったのではないだろうか・・・と思う．
}．
\subsubsection{3ワーカーの全員が一致する割合}
まず初めに，3ワーカー全員の意見が一致するのはどんな
割合だろうか．
7つのラベルについてすべて一致することは極めてまれであると
考えられるため，
あるタスクのあるラベルについて3人全員が一致する割合を計算してみよう．

あるタスクのあるラベルについて，3ワーカーの全員の回答が一致する
割合を計算すると，{\bf 57\% }であった，

これは一見高そうに見える．しかし，
これは「ラベルなし」で一致している場合も含まれる．
前提条件はラベルを与える場合について考えられる問題であり，
57\%のうち何割がラベルを与えた場合なのかを計算する必要がある．

計算するとこれは57\%のうちの{\bf12\%}に過ぎないことがわかった．
これは全体の1.1\%に近い．
\subsubsection{3ワーカーのうち2人が一致する割合}
次に，3ワーカーのうち多数の2人が一致するのはどんな割合だろうか．
同じようにあるタスクのあるラベルにおける一致率を考える．

自明なことだが，ラベルはありとなしに2択であり，3ワーカーのうち2人が一致する割合は
100\%である，
問題となるのはそのうち何割が「ラベルあり」とされているかである．

その割合を計算すると，こちらは100\%のうち，{\bf 2\%}ほどであった．
\subsubsection{前提条件は誤っている}
上記の検証から，「{\bf 人間が他人の顔に抱く印象は，
すべての人間で共通である．}」という前提条件は誤っているといえそうである．

今回のシステムはこの前提条件に依存しており，
そもそも目標を達成することははじめから不可能であったと考えられる．

機械学習がうまくいかない原因にはデータが不均衡である
ことも挙げられる．
しかし，図\ref{fig:ch3:label_uni}の分布から，
比較的均衡にラベルの有り無しが分布しているconfidentラベルにおいても，
図\ref{fig:ch3:binary}の結果においてF1-scoreが十分に高くないことから，
やはり，機械学習がうまくいかない原因は前提条件の誤りが大きいと考えられる．

課題設定の際には，その前提条件が誤っていないかを深く考えるべきであり，
これは今後の教訓になりそうだ．
\subsection{評価手法の難しさ}
前提条件の問題は重大であったが，
今回の実験においては評価手法にも問題があったように思える．

マルチラベル問題では適切な評価手法を選定することが難しい．
macro-F1は不均衡なデータにおいて適切でない場合があり，
Hamming Lossもその振る舞いが課題設定に適切であるかを考えることは難しい．

実際に，図\ref{fig:ch3:result}において，明らかに
学習がうまくいっていないマルチラベルモデルは，
二値分類モデルよりもHamming Lossが小さくなっている．
一方でmacro-F1はモデルの性能を正しく反映しているように見える．
一方で，実験1の結果である図\ref{fig:ch2:result}では，
macro-F1は両モデルでほぼ同じであり，
Hamming Lossのほうがモデル性能を正しく反映しているように見える．

今回の実験は．
マルチラベル問題において適切な評価指標を考えることがいかに難しいのかを
身を持って体験できたといえよう．
\subsection{なぜうまくいかなかったのか？：まとめ}
今回の実験がうまくいかず，テーマであったシステムを作成することが
できなかった原因は，第一に課題設定の前提条件，
「{\bf 人間が他人の顔に抱く印象は，
すべての人間で共通である．}」が誤っていたということ，加えて，
マルチラベル問題における評価手法が十分でなかったことが挙げられる．

学習データ不足や機械学習モデルの選定は，
上記の問題が解決されてから検討すべき項目であると考えられる．
\section{今後の課題}
今回の実験は残念ながら，課題設定の誤りから，
目的を達成することができなかった．

今後の課題として，まず考えられることは
テーマを見直すことである．

「人間が他人の顔に抱く印象は，
すべての人間で共通である．」という前提条件は誤りであった．
しかし，「ある人間が他人の顔に抱く印象」には一定の法則性が
ありそうである．例えば，すべての画像をAさんが1人でラベリング
したならば，「Aさん専用の」印象推定システムを作ることは可能
かもしれない．

あるいは最終発表のフィードバックでは，「顔ではなく目元に限定してみると
うまくいくかもしれない」といったコメントもいただいた．
個人的には，前提条件が誤っているという結果は少し違和感がある．
誰が見ても「明るい人」や「真面目そうな人」は
身の回りにもたくさんいるからだ．
認知科学系の研究成果などを深く調べれば，このように
顔ではなく目元など，対象を限定することで，
万人に共通するシステムを作れるかもしれない．

次に，マルチラベル問題に関するより良い機械学習での評価手法や
クラウドソージングでの結果統合手法・評価手法について学習することも
今後の課題といえる．
クラウドソージングで1タスク多数ワーカーとして
顔の印象を推定するタスクを行い，適切に結果を統合することができれば，
ある程度の傾向を掴んだ印象推定システムが作れる可能性もある．
\\

これらの今後の課題を踏まえ，これからの実験・研究活動にも
精力的に取り組んでいきたい．